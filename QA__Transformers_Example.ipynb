{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDeuGT9yyXpe",
        "outputId": "f34fed40-082f-430b-c598-aaee4557841c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "qa_dataset = [\n",
        "    {\n",
        "    'context': 'My name is AIVN and I am from Vietnam.',\n",
        "    'question': 'Where does AIVN come from?',\n",
        "    'answer': 'Vietnam'\n",
        "    },\n",
        "    {\n",
        "        'context': 'I love painting and my favorite artist is Van Gogh.',\n",
        "        'question': 'What is my favorite activity?',\n",
        "        'answer': 'painting'\n",
        "    },\n",
        "    {\n",
        "        'context': 'I am studying computer science at the University of Tokyo',\n",
        "        'question': 'Where do I live?',\n",
        "        'answer': 'Tokyo'\n",
        "    },\n",
        "    {\n",
        "        'context': 'I was born in Paris, but now I live in New York',\n",
        "        'question': 'Where do I live now',\n",
        "        'answer': 'New York'\n",
        "    },\n",
        "]\n",
        "\n",
        "data_size = len(qa_dataset)\n",
        "data_size\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tokenizer function\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for item in data_iter:\n",
        "        yield tokenizer('<cls> ' + item['context'] + ' <sep> ' + item['question'])\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = build_vocab_from_iterator(\n",
        "    yield_tokens(qa_dataset),\n",
        "    specials=['<unk>', '<pad>', '<bos>', '<eos>', '<sep>', '<cls>'])\n",
        "\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "vocab.get_stoi()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1KBvSCIybUk",
        "outputId": "8d43f56e-1c34-41e1-914a-9e9e77e7b953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'what': 45,\n",
              " 'was': 44,\n",
              " 'vietnam': 43,\n",
              " 'van': 42,\n",
              " 'university': 41,\n",
              " 'paris': 36,\n",
              " 'painting': 35,\n",
              " 'of': 34,\n",
              " 'new': 33,\n",
              " 'tokyo': 40,\n",
              " 'name': 32,\n",
              " 'love': 31,\n",
              " 'gogh': 30,\n",
              " 'does': 29,\n",
              " 'my': 10,\n",
              " '<sep>': 4,\n",
              " '<bos>': 2,\n",
              " 'is': 8,\n",
              " 'at': 24,\n",
              " 'science': 37,\n",
              " '?': 7,\n",
              " 'where': 11,\n",
              " '<cls>': 5,\n",
              " 'the': 39,\n",
              " 'in': 19,\n",
              " '<eos>': 3,\n",
              " 'come': 27,\n",
              " '<pad>': 1,\n",
              " 'computer': 28,\n",
              " '<unk>': 0,\n",
              " 'studying': 38,\n",
              " 'i': 6,\n",
              " 'aivn': 13,\n",
              " 'and': 15,\n",
              " 'artist': 23,\n",
              " 'favorite': 17,\n",
              " 'york': 46,\n",
              " 'am': 14,\n",
              " 'do': 16,\n",
              " 'now': 20,\n",
              " ',': 21,\n",
              " 'live': 9,\n",
              " 'activity': 22,\n",
              " '.': 12,\n",
              " 'born': 25,\n",
              " 'from': 18,\n",
              " 'but': 26}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pad_idx = vocab['<pad>']\n",
        "\n",
        "def pad_and_truncate(input_ids, max_length):\n",
        "    if len(input_ids) > max_length:\n",
        "        input_ids = input_ids[:max_length]\n",
        "    elif len(input_ids) < max_length:\n",
        "        input_ids += [vocab['<pad>']] * (max_length - len(input_ids))\n",
        "    return input_ids\n",
        "\n",
        "max_length = 30\n",
        "text = 'I love AIVN'\n",
        "tokens = tokenizer(text)\n",
        "input_ids = vocab(tokens)\n",
        "input_ids = pad_and_truncate(input_ids, max_length)\n",
        "input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkI2DI-bzSPF",
        "outputId": "4ac39459-b406-426c-9a20-9c6c8cc543f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6,\n",
              " 31,\n",
              " 13,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(question, context, answer):\n",
        "  input_text = question + ' <sep> ' + context\n",
        "  input_ids = [vocab[token] for token in tokenizer(input_text)]\n",
        "  input_ids = pad_and_truncate(input_ids, max_length)\n",
        "\n",
        "  answer_ids = [vocab[token] for token in tokenizer(answer)]\n",
        "  start_pos = input_ids.index(answer_ids[0])\n",
        "  end_pos = start_pos + len(answer_ids) - 1\n",
        "\n",
        "  input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "  start_pos = torch.tensor(start_pos, dtype=torch.long)\n",
        "  end_pos = torch.tensor(end_pos, dtype=torch.long)\n",
        "\n",
        "  return input_ids, start_pos, end_pos\n",
        "\n",
        "vectorize (\n",
        "    'What is your name?',\n",
        "    'My name is AIVN',\n",
        "    'AIVN'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbZddfCg3vXn",
        "outputId": "cd7173fa-6a6b-4029-ddcc-e7742981f51c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([45,  8,  0, 32,  7,  4, 10, 32,  8, 13,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]),\n",
              " tensor(9),\n",
              " tensor(9))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QADataset(Dataset):\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.data[idx]\n",
        "    question = item['question']\n",
        "    context = item['context']\n",
        "    answer = item['answer']\n",
        "\n",
        "    input_ids, start_pos, end_pos = vectorize(question, context, answer)\n",
        "\n",
        "\n",
        "\n",
        "    return input_ids, start_pos, end_pos"
      ],
      "metadata": {
        "id": "8wXypB0k8Fxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(input_ids):\n",
        "  return ' '.join([vocab.lookup_token(token) for token in input_ids])"
      ],
      "metadata": {
        "id": "1DfYXxbY8Jb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = QADataset(qa_dataset)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "QwJba9Ec8NZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))\n",
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddbNHfIi8RJu",
        "outputId": "bd4b5108-8251-4573-d5c1-4aa6f9cac0b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[11, 16,  6,  9,  7,  4,  6, 14, 38, 28, 37, 24, 39, 41, 34, 40,  1,  1,\n",
              "           1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
              "         [45,  8, 10, 17, 22,  7,  4,  6, 31, 35, 15, 10, 17, 23,  8, 42, 30, 12,\n",
              "           1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]]),\n",
              " tensor([15,  9]),\n",
              " tensor([15,  9])]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__ (self, embed_dim, num_heads, ff_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.ffn = nn.Linear(in_features=embed_dim,\n",
        "                             out_features=ff_dim)\n",
        "        self.layernorm_1 = nn.LayerNorm(normalized_shape=embed_dim)\n",
        "        self.layernorm_2 = nn.LayerNorm(normalized_shape=embed_dim)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        attn_output, _ = self.attn(query, key, value)\n",
        "        out_1 = self.layernorm_1(query + attn_output)\n",
        "        ffn_output = self.ffn(out_1)\n",
        "        x = self.layernorm_2(out_1 + ffn_output)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cGAzH3Rc3zC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__ (self, d_model, max_len=5000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.pe[:x.size(0), :]\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "D112EKNF5ic-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QAModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, n_heads, ff_dim, max_len):\n",
        "    super(QAModel, self).__init__()\n",
        "    self.input_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.positional_encoding = PositionalEncoding(embedding_dim, max_len)\n",
        "    self.transformer = TransformerBlock(embedding_dim, n_heads, ff_dim)\n",
        "\n",
        "    self.start_linear = nn.Linear(ff_dim, 1)\n",
        "    self.end_linear = nn.Linear(ff_dim, 1)\n",
        "\n",
        "  def forward(self, text):\n",
        "    input_embeded = self.input_embedding(text)\n",
        "    input_embeded = self.positional_encoding(input_embeded)\n",
        "    transformer_out = self.transformer(input_embeded, input_embeded, input_embeded)\n",
        "    start_logits = self.start_linear(transformer_out).squeeze(-1)\n",
        "    end_logits = self.end_linear(transformer_out).squeeze(-1)\n",
        "\n",
        "    return start_logits, end_logits"
      ],
      "metadata": {
        "id": "7zUekxaH6P5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model params\n",
        "Embedding_dims = 128\n",
        "FF_dim = 128\n",
        "N_heads = 1\n",
        "vocab_size = len(vocab.get_stoi().values())\n",
        "\n",
        "model = QAModel(vocab_size, Embedding_dims, N_heads, FF_dim, max_length)\n",
        "\n",
        "input = torch.randint(0, 10, size=(1, 10))\n",
        "print (input.shape)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  start_logit, end_logit = model(input)\n",
        "print (\"Shape of start logits\", start_logit.shape)\n",
        "print (\"Shape of end logits\", end_logit.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC4MN9Kc7M8w",
        "outputId": "c02e48db-b0e3-422a-bece-d77fad424727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10])\n",
            "Shape of start logits torch.Size([1, 10])\n",
            "Shape of end logits torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "EPOCHS = 15"
      ],
      "metadata": {
        "id": "hj5jE0QN7h_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "  for idx, (input_ids, start_pos, end_pos) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    start_logit, end_logit = model(input_ids)\n",
        "    start_loss = criterion(start_logit, start_pos)\n",
        "    end_loss = criterion(end_logit, end_pos)\n",
        "    loss = (start_loss + end_loss) / 2\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print (f'Epoch: {epoch+1}/{EPOCHS}, Batch: {idx+1}/{len(train_loader)}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qsgVNIP7xSl",
        "outputId": "1ad809cb-6e12-4df2-f312-abdfe7dadbc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/15, Batch: 1/2, Loss: 3.691862106323242\n",
            "Epoch: 1/15, Batch: 2/2, Loss: 3.517197608947754\n",
            "Epoch: 2/15, Batch: 1/2, Loss: 3.0236682891845703\n",
            "Epoch: 2/15, Batch: 2/2, Loss: 2.997326374053955\n",
            "Epoch: 3/15, Batch: 1/2, Loss: 2.7603917121887207\n",
            "Epoch: 3/15, Batch: 2/2, Loss: 2.162611961364746\n",
            "Epoch: 4/15, Batch: 1/2, Loss: 2.1418211460113525\n",
            "Epoch: 4/15, Batch: 2/2, Loss: 1.6836179494857788\n",
            "Epoch: 5/15, Batch: 1/2, Loss: 1.5318635702133179\n",
            "Epoch: 5/15, Batch: 2/2, Loss: 1.2931123971939087\n",
            "Epoch: 6/15, Batch: 1/2, Loss: 1.0562162399291992\n",
            "Epoch: 6/15, Batch: 2/2, Loss: 0.8903276324272156\n",
            "Epoch: 7/15, Batch: 1/2, Loss: 0.6317712068557739\n",
            "Epoch: 7/15, Batch: 2/2, Loss: 0.5451221466064453\n",
            "Epoch: 8/15, Batch: 1/2, Loss: 0.9038580656051636\n",
            "Epoch: 8/15, Batch: 2/2, Loss: 0.39669689536094666\n",
            "Epoch: 9/15, Batch: 1/2, Loss: 0.2213417887687683\n",
            "Epoch: 9/15, Batch: 2/2, Loss: 0.08456418663263321\n",
            "Epoch: 10/15, Batch: 1/2, Loss: 0.2979718744754791\n",
            "Epoch: 10/15, Batch: 2/2, Loss: 0.20126160979270935\n",
            "Epoch: 11/15, Batch: 1/2, Loss: 0.16878697276115417\n",
            "Epoch: 11/15, Batch: 2/2, Loss: 0.12261997163295746\n",
            "Epoch: 12/15, Batch: 1/2, Loss: 0.10340926051139832\n",
            "Epoch: 12/15, Batch: 2/2, Loss: 0.06845704466104507\n",
            "Epoch: 13/15, Batch: 1/2, Loss: 0.05414074286818504\n",
            "Epoch: 13/15, Batch: 2/2, Loss: 0.03863886371254921\n",
            "Epoch: 14/15, Batch: 1/2, Loss: 0.030040662735700607\n",
            "Epoch: 14/15, Batch: 2/2, Loss: 0.02580946870148182\n",
            "Epoch: 15/15, Batch: 1/2, Loss: 0.05232543125748634\n",
            "Epoch: 15/15, Batch: 2/2, Loss: 0.027803480625152588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  sample = qa_dataset[3]\n",
        "  context, question, answer = sample.values()\n",
        "  input_ids, start_pos, end_pos = vectorize(question, context, answer)\n",
        "  input_ids = input_ids.unsqueeze(0)\n",
        "  start_logit, end_logit = model(input_ids)\n",
        "\n",
        "  offset = len(tokenizer(question)) + 1\n",
        "\n",
        "  start_position = torch.argmax(start_logit, dim=1).numpy()[0]\n",
        "  end_position = torch.argmax(end_logit, dim=1).numpy()[0]\n",
        "\n",
        "  start_position -= offset\n",
        "  end_position -= offset\n",
        "\n",
        "  start_position = max(start_position, 0)\n",
        "  end_position = min(end_position, len(tokenizer(context)) - 1)\n",
        "\n",
        "  if end_position >= start_position:\n",
        "    # Extracted the predicted answer span\n",
        "    context_tokens = tokenizer(context)\n",
        "    predicted_answer = ' '.join(context_tokens[start_position:end_position+1])\n",
        "  else:\n",
        "    predicted_answer = \"\"\n",
        "\n",
        "  print(f\"Question: {question}\")\n",
        "  print(f\"Context: {context}\")\n",
        "  print (f\"Start Position: {start_position}\")\n",
        "  print (f\"End Position: {end_position}\")\n",
        "  print(f\"Predicted Answer: {predicted_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYGwF-RJ704S",
        "outputId": "52a8d7c6-8c5e-4e9f-c09a-267739bc7827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Where do I live now\n",
            "Context: I was born in Paris, but now I live in New York\n",
            "Start Position: 11\n",
            "End Position: 12\n",
            "Predicted Answer: new york\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lLd1JkLI8dtr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}